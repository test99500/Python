{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train = pd.read_csv('train_1.csv').fillna(0)\n",
    "print(train.head())\n",
    "\n",
    "def parse_page(page):\n",
    "    x = page.split('_')\n",
    "    return ' '.join(x[:-3]), x[-3], x[-2], x[-1]\n",
    "\n",
    "\n",
    "l = list(train.Page.apply(parse_page))\n",
    "df = pd.DataFrame(l)\n",
    "del l\n",
    "df.columns = ['Subject','Sub_Page','Access','Agent']\n",
    "df.head()\n",
    "\n",
    "train = pd.concat([train,df],axis=1)\n",
    "del train['Page']\n",
    "del df\n",
    "\n",
    "\n",
    "import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def lag_arr(arr, lag,fill):\n",
    "    filler = np.full((arr.shape[0],lag,1),-1)\n",
    "    comb = np.concatenate((filler,arr),axis=1)\n",
    "    result = comb[:,:arr.shape[1]]\n",
    "    return result\n",
    "\n",
    "\n",
    "def single_autocorr(series, lag):\n",
    "    \"\"\"\n",
    "    Autocorrelation for single data series\n",
    "    :param series: traffic series\n",
    "    :param lag: lag, days\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    s1 = series[lag:]\n",
    "    s2 = series[:-lag]\n",
    "    ms1 = np.mean(s1)\n",
    "    ms2 = np.mean(s2)\n",
    "    ds1 = s1 - ms1\n",
    "    ds2 = s2 - ms2\n",
    "    divider = np.sqrt(np.sum(ds1 * ds1)) * np.sqrt(np.sum(ds2 * ds2))\n",
    "    return np.sum(ds1 * ds2) / divider if divider != 0 else 0\n",
    "\n",
    "\n",
    "def batc_autocorr(data,lag,series_length):\n",
    "    corrs = []\n",
    "    for i in range(data.shape[0]):\n",
    "        c = single_autocorr(data, lag)\n",
    "        corrs.append(c)\n",
    "    corr = np.array(corrs)\n",
    "    corr = corr.reshape(-1,1)\n",
    "    corr = np.expand_dims(corr,-1)\n",
    "    corr = np.repeat(corr,series_length,axis=1)\n",
    "    return corr\n",
    "\n",
    "\n",
    "datetime.datetime.strptime(train.columns.values[0], '%Y-%m-%d').strftime('%a')\n",
    "weekdays = [datetime.datetime.strptime(date, '%Y-%m-%d').strftime('%a')\n",
    "            for date in train.columns.values[:-4]]\n",
    "\n",
    "day_one_hot = LabelEncoder().fit_transform(weekdays)\n",
    "day_one_hot = day_one_hot.reshape(-1, 1)\n",
    "day_one_hot = OneHotEncoder(sparse=False).fit_transform(day_one_hot)\n",
    "day_one_hot = np.expand_dims(day_one_hot,0)\n",
    "\n",
    "agent_int = LabelEncoder().fit(train['Agent'])\n",
    "agent_enc = agent_int.transform(train['Agent'])\n",
    "agent_enc = agent_enc.reshape(-1, 1)\n",
    "agent_one_hot = OneHotEncoder(sparse=False).fit(agent_enc)\n",
    "\n",
    "del agent_enc\n",
    "\n",
    "page_int = LabelEncoder().fit(train['Sub_Page'])\n",
    "page_enc = page_int.transform(train['Sub_Page'])\n",
    "page_enc = page_enc.reshape(-1, 1)\n",
    "page_one_hot = OneHotEncoder(sparse=False).fit(page_enc)\n",
    "\n",
    "del page_enc\n",
    "\n",
    "acc_int = LabelEncoder().fit(train['Access'])\n",
    "acc_enc = acc_int.transform(train['Access'])\n",
    "acc_enc = acc_enc.reshape(-1, 1)\n",
    "acc_one_hot = OneHotEncoder(sparse=False).fit(acc_enc)\n",
    "\n",
    "del acc_enc\n",
    "\n",
    "\n",
    "def get_batch(train,start=0,lookback = 100):\n",
    "    assert((start + lookback) <= (train.shape[1] - 5)) , 'End of lookback would be out of bounds'\n",
    "\n",
    "    data = train.iloc[:,start:start + lookback].values\n",
    "    target = train.iloc[:,start + lookback].values\n",
    "    target = np.log1p(target)\n",
    "\n",
    "    log_view = np.log1p(data)\n",
    "    log_view = np.expand_dims(log_view,axis=-1)\n",
    "\n",
    "    days = day_one_hot[:,start:start + lookback]\n",
    "    days = np.repeat(days,repeats=train.shape[0],axis=0)\n",
    "\n",
    "    year_lag = lag_arr(log_view,365,-1)\n",
    "    halfyear_lag = lag_arr(log_view,182,-1)\n",
    "    quarter_lag = lag_arr(log_view,91,-1)\n",
    "\n",
    "    agent_enc = agent_int.transform(train['Agent'])\n",
    "    agent_enc = agent_enc.reshape(-1, 1)\n",
    "    agent_enc = agent_one_hot.transform(agent_enc)\n",
    "    agent_enc = np.expand_dims(agent_enc,1)\n",
    "    agent_enc = np.repeat(agent_enc,lookback,axis=1)\n",
    "\n",
    "    page_enc = page_int.transform(train['Sub_Page'])\n",
    "    page_enc = page_enc.reshape(-1, 1)\n",
    "    page_enc = page_one_hot.transform(page_enc)\n",
    "    page_enc = np.expand_dims(page_enc, 1)\n",
    "    page_enc = np.repeat(page_enc,lookback,axis=1)\n",
    "\n",
    "    acc_enc = acc_int.transform(train['Access'])\n",
    "    acc_enc = acc_enc.reshape(-1, 1)\n",
    "    acc_enc = acc_one_hot.transform(acc_enc)\n",
    "    acc_enc = np.expand_dims(acc_enc,1)\n",
    "    acc_enc = np.repeat(acc_enc,lookback,axis=1)\n",
    "\n",
    "    year_autocorr = batc_autocorr(data,lag=365,series_length=lookback)\n",
    "    halfyr_autocorr = batc_autocorr(data,lag=182,series_length=lookback)\n",
    "    quarter_autocorr = batc_autocorr(data,lag=91,series_length=lookback)\n",
    "\n",
    "    medians = np.median(data,axis=1)\n",
    "    medians = np.expand_dims(medians,-1)\n",
    "    medians = np.expand_dims(medians,-1)\n",
    "    medians = np.repeat(medians,lookback,axis=1)\n",
    "\n",
    "\n",
    "    '''\n",
    "    print(log_view.shape)\n",
    "    print(days.shape)\n",
    "    print(year_lag.shape)\n",
    "    print(halfyear_lag.shape)\n",
    "    print(page_enc.shape)\n",
    "    print(agent_enc.shape)\n",
    "    print(acc_enc.shape)'''\n",
    "\n",
    "    batch = np.concatenate((log_view,\n",
    "                            days,\n",
    "                            year_lag,\n",
    "                            halfyear_lag,\n",
    "                            quarter_lag,\n",
    "                            page_enc,\n",
    "                            agent_enc,\n",
    "                            acc_enc,\n",
    "                            year_autocorr,\n",
    "                            halfyr_autocorr,\n",
    "                            quarter_autocorr,\n",
    "                            medians),axis=2)\n",
    "\n",
    "    return batch, target\n",
    "\n",
    "\n",
    "def generate_batches(train,batch_size = 32, lookback = 100):\n",
    "    num_samples = train.shape[0]\n",
    "    num_steps = train.shape[1] - 5\n",
    "    while True:\n",
    "        for i in range(num_samples // batch_size):\n",
    "            batch_start = i * batch_size\n",
    "            batch_end = batch_start + batch_size\n",
    "\n",
    "            seq_start = np.random.randint(num_steps - lookback)\n",
    "            X,y = get_batch(train.iloc[batch_start:batch_end],start=seq_start)\n",
    "            yield X,y\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.losses import mean_absolute_percentage_error\n",
    "from tensorflow.keras.layers import Conv1D, MaxPool1D, Dense, Activation, GlobalMaxPool1D, Flatten\n",
    "\n",
    "number_of_features = 29\n",
    "max_length = 100\n",
    "\n",
    "model = Sequential([Conv1D(filters=16, kernel_size=5, input_shape=(100, 29), activation='relu'),\n",
    "                    MaxPool1D(pool_size=5),\n",
    "                    Conv1D(filters=16, kernel_size=5, activation='relu'),\n",
    "                    MaxPool1D(5),\n",
    "                    Flatten(),\n",
    "                    Dense(units=1)])\n",
    "\n",
    "model.compile(optimizer='adam', loss=mean_absolute_percentage_error)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "batch_size = 128\n",
    "train_df, val_df = train_test_split(train, test_size=0.1)\n",
    "train_gen = generate_batches(train_df,batch_size=batch_size)\n",
    "val_gen = generate_batches(val_df, batch_size=batch_size)\n",
    "\n",
    "n_train_samples = train_df.shape[0]\n",
    "n_val_samples = val_df.shape[0]\n",
    "\n",
    "a,b = next(train_gen)\n",
    "\n",
    "model.fit_generator(train_gen,\n",
    "                    epochs=1,\n",
    "                    steps_per_epoch=n_train_samples // batch_size,\n",
    "                    validation_data= val_gen,\n",
    "                    validation_steps=n_val_samples // batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}